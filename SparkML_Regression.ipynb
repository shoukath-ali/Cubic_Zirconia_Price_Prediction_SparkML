{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e909108-6218-4f5c-9525-1b70bd605840",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pyspark\n",
      "  Downloading pyspark-3.5.3.tar.gz (317.3 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m120.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py4j==0.10.9.7 (from pyspark)\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.3-py2.py3-none-any.whl size=317840629 sha256=72be29c711fb8f4a2d43c7ec7b772ec983eea6cebcd83713226f7d5845fbab05\n",
      "  Stored in directory: /home/exouser/.cache/pip/wheels/07/a0/a3/d24c94bf043ab5c7e38c30491199a2a11fef8d2584e6df7fb7\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.5.3\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "095d05f2-510d-439f-afa8-758bc0351887",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed488c16-99ee-41cc-a03b-078b38fd51c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e2fd7868-c4dd-4def-8593-e288301f8ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting findspark\n",
      "  Downloading findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
      "Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-2.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f156eaba-c200-401e-beb4-035e3ee02fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d5ff4d-721c-4b44-a494-fa2b7d8b50c3",
   "metadata": {},
   "source": [
    "### 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2ac5215-9b33-4097-9748-eae3a39ce5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/27 14:56:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder\\\n",
    "        .appName('Assignment2').\\\n",
    "        config( 'spark.executor.instances','2'). \\\n",
    "        config('spark.executor.cores','2'). \\\n",
    "        config('spark.executor-memory', '3g').\\\n",
    "        enableHiveSupport()\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5697c3f3-aac4-4b5e-8d42-36856744644b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://bigdata-instance-ali.js2local:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Assignment2</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x781d0c143290>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01115db7-0baf-4ba2-8ead-46b04ad569f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/27 14:56:41 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "spark._jsc.sc().getExecutorMemoryStatus().size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bc6b73a-3f93-468f-9d9d-7a68915f82a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cubzon= spark.read\\\n",
    "        .format('csv')\\\n",
    "        .option(\"header\",\"true\")\\\n",
    "        .option(\"inferschema\",\"true\")\\\n",
    "        .load(\"/home/exouser/bigdata/cubic_zirconia.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ddb25c4-750b-4a2c-87f3-56cfc25f7765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---------+-----+-------+-----+-----+----+----+----+-----+\n",
      "|_c0|carat|      cut|color|clarity|depth|table|   x|   y|   z|price|\n",
      "+---+-----+---------+-----+-------+-----+-----+----+----+----+-----+\n",
      "|  1|  0.3|    Ideal|    E|    SI1| 62.1| 58.0|4.27|4.29|2.66|  499|\n",
      "|  2| 0.33|  Premium|    G|     IF| 60.8| 58.0|4.42|4.46| 2.7|  984|\n",
      "|  3|  0.9|Very Good|    E|   VVS2| 62.2| 60.0|6.04|6.12|3.78| 6289|\n",
      "|  4| 0.42|    Ideal|    F|    VS1| 61.6| 56.0|4.82| 4.8|2.96| 1082|\n",
      "|  5| 0.31|    Ideal|    F|   VVS1| 60.4| 59.0|4.35|4.43|2.65|  779|\n",
      "|  6| 1.02|    Ideal|    D|    VS2| 61.5| 56.0|6.46|6.49|3.99| 9502|\n",
      "|  7| 1.01|     Good|    H|    SI1| 63.7| 60.0|6.35| 6.3|4.03| 4836|\n",
      "|  8|  0.5|  Premium|    E|    SI1| 61.5| 62.0|5.09|5.06|3.12| 1415|\n",
      "|  9| 1.21|     Good|    H|    SI1| 63.8| 64.0|6.72|6.63|4.26| 5407|\n",
      "| 10| 0.35|    Ideal|    F|    VS2| 60.5| 57.0|4.52| 4.6|2.76|  706|\n",
      "| 11| 0.32|    Ideal|    E|    VS2| 61.6| 56.0| 4.4|4.43|2.72|  637|\n",
      "| 12|  1.1|  Premium|    D|    SI1| 60.7| 55.0|6.74|6.71|4.08| 6468|\n",
      "| 13|  0.5|     Good|    E|    VS1| 61.1| 58.2|5.08|5.12|3.11| 1932|\n",
      "| 14| 0.71|    Ideal|    D|    SI2| 61.6| 55.0|5.74|5.76|3.54| 2767|\n",
      "| 15|  1.5|     Fair|    G|    VS2| 66.2| 53.0|7.12|7.08| 4.7|10644|\n",
      "| 16| 0.31|    Ideal|    G|    VS2| 61.6| 55.0|4.37|4.39| 2.7|  544|\n",
      "| 17| 0.34|    Ideal|    G|    SI1| 61.2| 57.0|4.56|4.53|2.78|  650|\n",
      "| 18| 1.01|    Ideal|    D|    VS2| 59.8| 56.0|6.52|6.49|3.89| 7127|\n",
      "| 19|  0.9|     Good|    D|    SI1| 61.9| 64.0| 6.0|6.09|3.74| 3567|\n",
      "| 20| 0.54|  Premium|    G|    VS2| 60.0| 59.0|5.42|5.22|3.19| 1637|\n",
      "+---+-----+---------+-----+-------+-----+-----+----+----+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/27 14:56:46 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , carat, cut, color, clarity, depth, table, x, y, z, price\n",
      " Schema: _c0, carat, cut, color, clarity, depth, table, x, y, z, price\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/exouser/bigdata/cubic_zirconia.csv\n"
     ]
    }
   ],
   "source": [
    "df_cubzon.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0fdcd7-cd4b-4b70-a231-9e11331f10a4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Part1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6505f1f1-030a-4a51-95b8-c4105de8124e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### 1 Data Loading and Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b74d0102-9a99-419d-80f4-5d38b2c5293b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---------+-----+-------+-----+-----+----+----+----+-----+\n",
      "|_c0|carat|      cut|color|clarity|depth|table|   x|   y|   z|price|\n",
      "+---+-----+---------+-----+-------+-----+-----+----+----+----+-----+\n",
      "|  1|  0.3|    Ideal|    E|    SI1| 62.1| 58.0|4.27|4.29|2.66|  499|\n",
      "|  2| 0.33|  Premium|    G|     IF| 60.8| 58.0|4.42|4.46| 2.7|  984|\n",
      "|  3|  0.9|Very Good|    E|   VVS2| 62.2| 60.0|6.04|6.12|3.78| 6289|\n",
      "|  4| 0.42|    Ideal|    F|    VS1| 61.6| 56.0|4.82| 4.8|2.96| 1082|\n",
      "|  5| 0.31|    Ideal|    F|   VVS1| 60.4| 59.0|4.35|4.43|2.65|  779|\n",
      "+---+-----+---------+-----+-------+-----+-----+----+----+----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/27 15:33:56 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , carat, cut, color, clarity, depth, table, x, y, z, price\n",
      " Schema: _c0, carat, cut, color, clarity, depth, table, x, y, z, price\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/exouser/bigdata/cubic_zirconia.csv\n"
     ]
    }
   ],
   "source": [
    "df_cubzon.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a2d163c-1432-4ee8-aeb4-bca427357fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---------+-----+-------+-----+-------------+------+-----+------+-----+\n",
      "|_c0|Carat|      Cut|Color|Clarity|depth|Table_percent|Length|Width|Height|Price|\n",
      "+---+-----+---------+-----+-------+-----+-------------+------+-----+------+-----+\n",
      "|  1|  0.3|    Ideal|    E|    SI1| 62.1|         58.0|  4.27| 4.29|  2.66|  499|\n",
      "|  2| 0.33|  Premium|    G|     IF| 60.8|         58.0|  4.42| 4.46|   2.7|  984|\n",
      "|  3|  0.9|Very Good|    E|   VVS2| 62.2|         60.0|  6.04| 6.12|  3.78| 6289|\n",
      "|  4| 0.42|    Ideal|    F|    VS1| 61.6|         56.0|  4.82|  4.8|  2.96| 1082|\n",
      "|  5| 0.31|    Ideal|    F|   VVS1| 60.4|         59.0|  4.35| 4.43|  2.65|  779|\n",
      "|  6| 1.02|    Ideal|    D|    VS2| 61.5|         56.0|  6.46| 6.49|  3.99| 9502|\n",
      "|  7| 1.01|     Good|    H|    SI1| 63.7|         60.0|  6.35|  6.3|  4.03| 4836|\n",
      "|  8|  0.5|  Premium|    E|    SI1| 61.5|         62.0|  5.09| 5.06|  3.12| 1415|\n",
      "|  9| 1.21|     Good|    H|    SI1| 63.8|         64.0|  6.72| 6.63|  4.26| 5407|\n",
      "| 10| 0.35|    Ideal|    F|    VS2| 60.5|         57.0|  4.52|  4.6|  2.76|  706|\n",
      "| 11| 0.32|    Ideal|    E|    VS2| 61.6|         56.0|   4.4| 4.43|  2.72|  637|\n",
      "| 12|  1.1|  Premium|    D|    SI1| 60.7|         55.0|  6.74| 6.71|  4.08| 6468|\n",
      "| 13|  0.5|     Good|    E|    VS1| 61.1|         58.2|  5.08| 5.12|  3.11| 1932|\n",
      "| 14| 0.71|    Ideal|    D|    SI2| 61.6|         55.0|  5.74| 5.76|  3.54| 2767|\n",
      "| 15|  1.5|     Fair|    G|    VS2| 66.2|         53.0|  7.12| 7.08|   4.7|10644|\n",
      "| 16| 0.31|    Ideal|    G|    VS2| 61.6|         55.0|  4.37| 4.39|   2.7|  544|\n",
      "| 17| 0.34|    Ideal|    G|    SI1| 61.2|         57.0|  4.56| 4.53|  2.78|  650|\n",
      "| 18| 1.01|    Ideal|    D|    VS2| 59.8|         56.0|  6.52| 6.49|  3.89| 7127|\n",
      "| 19|  0.9|     Good|    D|    SI1| 61.9|         64.0|   6.0| 6.09|  3.74| 3567|\n",
      "| 20| 0.54|  Premium|    G|    VS2| 60.0|         59.0|  5.42| 5.22|  3.19| 1637|\n",
      "+---+-----+---------+-----+-------+-----+-------------+------+-----+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/27 15:33:57 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , carat, cut, color, clarity, depth, table, x, y, z, price\n",
      " Schema: _c0, carat, cut, color, clarity, depth, table, x, y, z, price\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/exouser/bigdata/cubic_zirconia.csv\n"
     ]
    }
   ],
   "source": [
    "df_cubzon = df_cubzon.withColumnsRenamed({'carat' :'Carat', 'cut' :'Cut', 'color' :'Color' ,\\\n",
    "                             'clarity' :'Clarity','dept' :'Dept_percent','table':'Table_percent' \\\n",
    "                            , 'x' :'Length', 'y': 'Width','z': 'Height', 'price' :'Price'})\n",
    "df_cubzon.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2574f853-0fd5-466a-83e5-d57397c2f98b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### 2 Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f0ec429-089b-4118-b851-fab043149b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows  26967 number of columns 11\n"
     ]
    }
   ],
   "source": [
    "print(\"number of rows \",df_cubzon.count(), \"number of columns\",len(df_cubzon.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85f4e977-0b06-4022-b585-f516f6c44437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- Carat: double (nullable = true)\n",
      " |-- Cut: string (nullable = true)\n",
      " |-- Color: string (nullable = true)\n",
      " |-- Clarity: string (nullable = true)\n",
      " |-- depth: double (nullable = true)\n",
      " |-- Table_percent: double (nullable = true)\n",
      " |-- Length: double (nullable = true)\n",
      " |-- Width: double (nullable = true)\n",
      " |-- Height: double (nullable = true)\n",
      " |-- Price: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# data types for each column\n",
    "df_cubzon.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52b77c2f-7aca-4966-af0c-30e1c4cae734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|      Cut|\n",
      "+---------+\n",
      "|  Premium|\n",
      "|    Ideal|\n",
      "|     Good|\n",
      "|     Fair|\n",
      "|Very Good|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Unique values in Cut\n",
    "df_cubzon.select('Cut').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed8356a9-3e34-441d-ba41-3b12fa60c8c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|Color|\n",
      "+-----+\n",
      "|    F|\n",
      "|    E|\n",
      "|    D|\n",
      "|    J|\n",
      "|    G|\n",
      "|    I|\n",
      "|    H|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Unique values in COLOR\n",
    "df_cubzon.select('Color').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4abd28f-f3da-406b-b546-7c817b8c09d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|Clarity|\n",
      "+-------+\n",
      "|   VVS2|\n",
      "|    SI1|\n",
      "|     IF|\n",
      "|     I1|\n",
      "|   VVS1|\n",
      "|    VS2|\n",
      "|    SI2|\n",
      "|    VS1|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Unique values in Clarity\n",
    "df_cubzon.select('Clarity').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7d050e-a5a2-4a62-9bec-be60f5cf0dd8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d725db3f-6a68-4144-bfe6-3ea93614ff34",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### 1 Handling Missing Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91af2eaf-0065-4193-acd3-48d6ed00f429",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/27 15:35:04 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , carat, cut, color, clarity, depth, table, x, y, z, price\n",
      " Schema: _c0, carat, cut, color, clarity, depth, table, x, y, z, price\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/exouser/bigdata/cubic_zirconia.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+-----+-------+-----+-------------+------+-----+------+-----+\n",
      "|_c0|Carat|Cut|Color|Clarity|depth|Table_percent|Length|Width|Height|Price|\n",
      "+---+-----+---+-----+-------+-----+-------------+------+-----+------+-----+\n",
      "|  0|    0|  0|    0|      0|  697|            0|     0|    0|     0|    0|\n",
      "+---+-----+---+-----+-------+-----+-------------+------+-----+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Missing values\n",
    "from pyspark.sql.functions import isnan, when, count, col, format_number\n",
    "nulls = df_cubzon.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_cubzon.columns])\n",
    "nulls.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b606b028-6631-4c06-8099-126c44ca2b54",
   "metadata": {},
   "source": [
    "This shows that depth has allot of null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5836834-4003-4a38-996f-a9270775e24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/27 15:35:09 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , carat, cut, color, clarity, depth, table, x, y, z, price\n",
      " Schema: _c0, carat, cut, color, clarity, depth, table, x, y, z, price\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/exouser/bigdata/cubic_zirconia.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+-----+-------+-----+-------------+------+-----+------+-----+\n",
      "|_c0|Carat|Cut|Color|Clarity|depth|Table_percent|Length|Width|Height|Price|\n",
      "+---+-----+---+-----+-------+-----+-------------+------+-----+------+-----+\n",
      "|0.0|  0.0|0.0|  0.0|    0.0|  2.6|          0.0|   0.0|  0.0|   0.0|  0.0|\n",
      "+---+-----+---+-----+-------+-----+-------------+------+-----+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## lets check the percentage and handle missing values\n",
    "\n",
    "percent = df_cubzon.select([format_number(((count(when(isnan(c) | col(c).isNull(), c))/df_cubzon.count())*100),1).alias(c) for c in df_cubzon.columns])\n",
    "percent.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b11ccd-aee1-47fa-8c8f-1a933f092f00",
   "metadata": {},
   "source": [
    "As the percent of missing values is very less, lets drop the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c684e14-a50a-4292-ade7-7c01d6b1e835",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cubzon_clean = df_cubzon.na.drop(subset = [\"depth\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc5cc8cd-4f8c-43fe-b0be-ab322477add0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26270"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cubzon_clean.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba62b0b-88fa-431e-b5fc-d58ca7a2275e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### 2. Descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4508bd0f-8098-41f9-a5c7-0a8d26e00090",
   "metadata": {},
   "outputs": [],
   "source": [
    "## organising numerical and string columns\n",
    "from pyspark.sql.types import *\n",
    "num_ip = []\n",
    "str_ip = []\n",
    "for col in df_cubzon.columns:\n",
    "    if str(df_cubzon_clean.schema[col].dataType) == 'StringType()':\n",
    "        str_ip.append(col)\n",
    "    else:\n",
    "        num_ip.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d618b266-bb45-4f42-9ce0-a97f4df7ccd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_c0',\n",
       " 'Carat',\n",
       " 'depth',\n",
       " 'Table_percent',\n",
       " 'Length',\n",
       " 'Width',\n",
       " 'Height',\n",
       " 'Price']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_ip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "32faf99b-43d8-4628-bfd6-5ca6093bf292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Carat', 'depth', 'Table_percent', 'Length', 'Width', 'Height']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## remove index column\n",
    "num_ip = num_ip[1:-1]\n",
    "num_ip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b1f23222-8d4f-410d-bf88-5b60e45d1d02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cut', 'Color', 'Clarity']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_ip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "adc81402-da82-4f9f-a383-e9924d6d2025",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/27 15:36:50 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|summary|Carat              |depth             |Table_percent     |Length            |Width             |Height            |Price             |\n",
      "+-------+-------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|count  |26270              |26270             |26270             |26270             |26270             |26270             |26270             |\n",
      "|mean   |0.7979950513894071 |61.745146555006194|57.45601065854585 |5.7291674914351045|5.732967263037647 |3.53663608679101  |3937.9737342976778|\n",
      "|stddev |0.47721435982056454|1.4128602381426116|2.2307763757680776|1.1275149511617892|1.1663302971035634|0.6993349197962537|4022.1893410338644|\n",
      "|min    |0.2                |50.8              |49.0              |0.0               |0.0               |0.0               |326               |\n",
      "|max    |4.5                |73.6              |79.0              |10.23             |58.9              |8.06              |18818             |\n",
      "+-------+-------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## summary statistics\n",
    "df_cubzon_clean[num_ip].describe().show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d88e627-3b26-4e3c-aede-42266325e326",
   "metadata": {},
   "source": [
    "Looking at the data ,Carat and Price: With carat weights ranging from 0.2 to 4.5, combined with prices from $326$ up to nearly $19,000$, there’s a likely relationship between size and price, though the variability in price suggests that other factors (like clarity, cut quality, and rarity) are at play as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "89309f85-9d6d-489d-af1e-edcb26c2a693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Carat': [0.2, 0.4, 4.5, 4.5],\n",
       " 'depth': [50.8, 61.0, 73.6, 73.6],\n",
       " 'Table_percent': [49.0, 56.0, 79.0, 79.0],\n",
       " 'Length': [0.0, 4.71, 10.23, 10.23],\n",
       " 'Width': [0.0, 4.72, 58.9, 58.9],\n",
       " 'Height': [0.0, 2.9, 8.06, 8.06]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### outliers\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "d = {}\n",
    "for col in num_ip:\n",
    "    d[col] = df_cubzon_clean.approxQuantile(col,[0.25,0.50,0.75,0.99],0.25)\n",
    "\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4ed17beb-031d-4ac1-a8db-20bb075a20a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carat has been treated for positive (right) skewness. (skew = 1.1176883595677425 )\n",
      "Width has been treated for positive (right) skewness. (skew = 3.9392923373162962 )\n"
     ]
    }
   ],
   "source": [
    "df = df_cubzon_clean\n",
    "for col in num_ip:\n",
    "    skew = df.agg(skewness(df[col])).collect()\n",
    "    skew = skew[0][0]\n",
    "    if skew > 1:\n",
    "        df = df.withColumn(col, \\\n",
    "        log(when(df[col]< d[col][0],d[col][0])\\\n",
    "        .when(df[col]>d[col][1],d[col][1])\\\n",
    "        .otherwise(df[col]) + 1)).alias(col)\n",
    "        print(col+\" has been treated for positive (right) skewness. (skew =\",skew,\")\")\n",
    "    elif skew < -1:\n",
    "        df = df.withColumn(col, \\\n",
    "        exp(when(df[col]< d[col][0],d[col][0])\\\n",
    "        .when(df[col]>d[col][1],d[col][1])\\\n",
    "        .otherwise(df[col]) + 1)).alias(col)\n",
    "        print(col+\" has been treated for negative (left) skewness. (skew =\",skew,\")\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec4874e-aa86-4697-b4aa-f49992d84120",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### 3.Categorical Value Distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "163f70fb-2bff-4fe6-9308-315bc38bf465",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "def dist_cal(col):\n",
    "    dist = df_cubzon_clean.groupBy(col).count()\n",
    "    total_count = df_cubzon_clean.count()\n",
    "    dist = dist.withColumn(\"dist_percent\",(F.col(\"count\")/total_count) * 100)\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ea86cd36-acae-42f4-8010-e202d26f78c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distribution for Cut\n",
      "+---------+-----+------------------+\n",
      "|      Cut|count|      dist_percent|\n",
      "+---------+-----+------------------+\n",
      "|  Premium| 6707|25.531023981728207|\n",
      "|    Ideal|10546|40.144651693947466|\n",
      "|     Good| 2382| 9.067377236391321|\n",
      "|     Fair|  757| 2.881614008374572|\n",
      "|Very Good| 5878| 22.37533307955843|\n",
      "+---------+-----+------------------+\n",
      "\n",
      "distribution for Color\n",
      "+-----+-----+------------------+\n",
      "|Color|count|      dist_percent|\n",
      "+-----+-----+------------------+\n",
      "|    F| 4612| 17.55614769699277|\n",
      "|    E| 4793| 18.24514655500571|\n",
      "|    D| 3268|  12.4400456794823|\n",
      "|    J| 1401| 5.333079558431671|\n",
      "|    G| 5529| 21.04682146935668|\n",
      "|    I| 2676|10.186524552721735|\n",
      "|    H| 3991|15.192234488009134|\n",
      "+-----+-----+------------------+\n",
      "\n",
      "distribution for Clarity\n",
      "+-------+-----+------------------+\n",
      "|Clarity|count|      dist_percent|\n",
      "+-------+-----+------------------+\n",
      "|   VVS2| 2479|  9.43661971830986|\n",
      "|    SI1| 6408|24.392843547773126|\n",
      "|     IF|  874|3.3269889607917777|\n",
      "|     I1|  355|1.3513513513513513|\n",
      "|   VVS1| 1791| 6.817662733155691|\n",
      "|    VS2| 5925|  22.5542443852303|\n",
      "|    SI2| 4447| 16.92805481537876|\n",
      "|    VS1| 3991|15.192234488009134|\n",
      "+-------+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for col in str_ip:\n",
    "    print(\"distribution for \"+col)\n",
    "    dist_cal(col).show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40949c72-6459-45a7-a837-e3a4f2d36987",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Part3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dafbff1-5ba7-49fc-9617-11a9900182b1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### 1. Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cad085ff-d664-4b17-a742-92169116bb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "df_cubzon_clean = df_cubzon_clean.withColumn(\"Volume\",(df_cubzon_clean[\"Length\"] * df_cubzon_clean[\"Width\"] * df_cubzon_clean[\"Height\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "64b8c53a-d331-452e-90aa-8f4e321cf181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---------+-----+-------+-----+-------------+------+-----+------+-----+------------------+\n",
      "|_c0|Carat|      Cut|Color|Clarity|depth|Table_percent|Length|Width|Height|Price|            Volume|\n",
      "+---+-----+---------+-----+-------+-----+-------------+------+-----+------+-----+------------------+\n",
      "|  1|  0.3|    Ideal|    E|    SI1| 62.1|         58.0|  4.27| 4.29|  2.66|  499| 48.72667799999999|\n",
      "|  2| 0.33|  Premium|    G|     IF| 60.8|         58.0|  4.42| 4.46|   2.7|  984|53.225640000000006|\n",
      "|  3|  0.9|Very Good|    E|   VVS2| 62.2|         60.0|  6.04| 6.12|  3.78| 6289|        139.726944|\n",
      "|  4| 0.42|    Ideal|    F|    VS1| 61.6|         56.0|  4.82|  4.8|  2.96| 1082| 68.48255999999999|\n",
      "|  5| 0.31|    Ideal|    F|   VVS1| 60.4|         59.0|  4.35| 4.43|  2.65|  779|51.066824999999994|\n",
      "|  6| 1.02|    Ideal|    D|    VS2| 61.5|         56.0|  6.46| 6.49|  3.99| 9502|167.28234600000002|\n",
      "|  7| 1.01|     Good|    H|    SI1| 63.7|         60.0|  6.35|  6.3|  4.03| 4836|         161.22015|\n",
      "|  8|  0.5|  Premium|    E|    SI1| 61.5|         62.0|  5.09| 5.06|  3.12| 1415|         80.356848|\n",
      "|  9| 1.21|     Good|    H|    SI1| 63.8|         64.0|  6.72| 6.63|  4.26| 5407|189.79833599999998|\n",
      "| 10| 0.35|    Ideal|    F|    VS2| 60.5|         57.0|  4.52|  4.6|  2.76|  706| 57.38591999999999|\n",
      "| 11| 0.32|    Ideal|    E|    VS2| 61.6|         56.0|   4.4| 4.43|  2.72|  637|53.018240000000006|\n",
      "| 12|  1.1|  Premium|    D|    SI1| 60.7|         55.0|  6.74| 6.71|  4.08| 6468|        184.519632|\n",
      "| 13|  0.5|     Good|    E|    VS1| 61.1|         58.2|  5.08| 5.12|  3.11| 1932| 80.88985600000001|\n",
      "| 14| 0.71|    Ideal|    D|    SI2| 61.6|         55.0|  5.74| 5.76|  3.54| 2767|117.04089599999999|\n",
      "| 15|  1.5|     Fair|    G|    VS2| 66.2|         53.0|  7.12| 7.08|   4.7|10644|236.92512000000002|\n",
      "| 16| 0.31|    Ideal|    G|    VS2| 61.6|         55.0|  4.37| 4.39|   2.7|  544|51.797610000000006|\n",
      "| 17| 0.34|    Ideal|    G|    SI1| 61.2|         57.0|  4.56| 4.53|  2.78|  650|57.425903999999996|\n",
      "| 18| 1.01|    Ideal|    D|    VS2| 59.8|         56.0|  6.52| 6.49|  3.89| 7127|        164.604572|\n",
      "| 19|  0.9|     Good|    D|    SI1| 61.9|         64.0|   6.0| 6.09|  3.74| 3567|          136.6596|\n",
      "| 20| 0.54|  Premium|    G|    VS2| 60.0|         59.0|  5.42| 5.22|  3.19| 1637| 90.25275599999999|\n",
      "+---+-----+---------+-----+-------+-----+-------------+------+-----+------+-----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/27 15:52:22 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , carat, cut, color, clarity, depth, table, x, y, z, price\n",
      " Schema: _c0, carat, cut, color, clarity, depth, table, x, y, z, price\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/exouser/bigdata/cubic_zirconia.csv\n"
     ]
    }
   ],
   "source": [
    "df_cubzon_clean.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0a6777-091e-4a81-90f3-d1f523c144ab",
   "metadata": {},
   "source": [
    "For diamonds, Volume is the best feature to predict the price, where as carat is the size of the diamond, volume and carat plays major role in defining the price of diamond."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01efae10-f0ac-40ee-9c1b-324860a69c89",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### 2.Feature Scaling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2238c853-c1c4-4ed7-95cb-b3db1809fa1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Carat', 'depth', 'Table_percent', 'Volume']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets clean the input outputs and generate features columns / vector\n",
    "num_ip.remove('Length')\n",
    "num_ip.remove('Width')\n",
    "num_ip.remove('Height')\n",
    "num_ip.append('Volume')\n",
    "num_ip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "40165651-e82a-443a-a2ae-dddaf3ed3d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Vector\n",
    "assembler = VectorAssembler(inputCols=num_ip, outputCol= 'features')\n",
    "output = assembler.transform(df_cubzon_clean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd357c2f-995e-44a2-a532-c5277380c774",
   "metadata": {},
   "source": [
    "Based on the features we have , Normalization is not required, as it scales b/w the range(0,1], this is used when we want to preserve zero. Where as standardiastion helps to center the data around the mean with standard deviation 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0371641e-8742-43e0-adc6-6972a7d3dd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler, StandardScaler\n",
    "scaler = StandardScaler(inputCol= 'features' , outputCol = 'standarad_features')\n",
    "standard_model = scaler.fit(output)\n",
    "std_data = standard_model.transform(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b6fe3edc-8e89-4ff4-8788-e8701debcbeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---------+-----+-------+-----+-------------+------+-----+------+-----+------------------+--------------------+--------------------+\n",
      "|_c0|Carat|      Cut|Color|Clarity|depth|Table_percent|Length|Width|Height|Price|            Volume|            features|  standarad_features|\n",
      "+---+-----+---------+-----+-------+-----+-------------+------+-----+------+-----+------------------+--------------------+--------------------+\n",
      "|  1|  0.3|    Ideal|    E|    SI1| 62.1|         58.0|  4.27| 4.29|  2.66|  499| 48.72667799999999|[0.3,62.1,58.0,48...|[0.62864830830489...|\n",
      "|  2| 0.33|  Premium|    G|     IF| 60.8|         58.0|  4.42| 4.46|   2.7|  984|53.225640000000006|[0.33,60.8,58.0,5...|[0.69151313913538...|\n",
      "|  3|  0.9|Very Good|    E|   VVS2| 62.2|         60.0|  6.04| 6.12|  3.78| 6289|        139.726944|[0.9,62.2,60.0,13...|[1.88594492491467...|\n",
      "|  4| 0.42|    Ideal|    F|    VS1| 61.6|         56.0|  4.82|  4.8|  2.96| 1082| 68.48255999999999|[0.42,61.6,56.0,6...|[0.88010763162684...|\n",
      "|  5| 0.31|    Ideal|    F|   VVS1| 60.4|         59.0|  4.35| 4.43|  2.65|  779|51.066824999999994|[0.31,60.4,59.0,5...|[0.64960325191505...|\n",
      "+---+-----+---------+-----+-------+-----+-------------+------+-----+------+-----+------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/27 15:54:47 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , carat, cut, color, clarity, depth, table, x, y, z, price\n",
      " Schema: _c0, carat, cut, color, clarity, depth, table, x, y, z, price\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/exouser/bigdata/cubic_zirconia.csv\n"
     ]
    }
   ],
   "source": [
    "std_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8ef3f7-d7a0-43a4-ac67-02a70f38b434",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### 3.Relationship Between Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5b090da6-f94d-41ec-b43b-0cf287e7bb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = {}\n",
    "for col in num_ip:\n",
    "    corr[col] = df_cubzon_clean.stat.corr(col,'Price')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "73504aa6-7c38-438e-826e-1df8bbf4ebe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Carat': 0.9221335932129359,\n",
       " 'depth': -0.002568616329956218,\n",
       " 'Table_percent': 0.1265870119852145,\n",
       " 'Volume': 0.8878757976746712}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a13c1de-30bd-47d3-bf03-491b4d13663a",
   "metadata": {},
   "source": [
    "Focusing on features such a Carat and Volume as it has a strong positive correlation and dept is negligible as it is close to zero and table_percent is weak positive correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "de0ecb98-0ff3-465a-990e-d8a15de8838d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---------+-----+-------+-----+-------------+------+-----+------+-----+------------------+--------------------+--------------------+\n",
      "|_c0|Carat|      Cut|Color|Clarity|depth|Table_percent|Length|Width|Height|Price|            Volume|            features|  standarad_features|\n",
      "+---+-----+---------+-----+-------+-----+-------------+------+-----+------+-----+------------------+--------------------+--------------------+\n",
      "|  1|  0.3|    Ideal|    E|    SI1| 62.1|         58.0|  4.27| 4.29|  2.66|  499| 48.72667799999999|[0.3,62.1,58.0,48...|[0.62864830830489...|\n",
      "|  2| 0.33|  Premium|    G|     IF| 60.8|         58.0|  4.42| 4.46|   2.7|  984|53.225640000000006|[0.33,60.8,58.0,5...|[0.69151313913538...|\n",
      "|  3|  0.9|Very Good|    E|   VVS2| 62.2|         60.0|  6.04| 6.12|  3.78| 6289|        139.726944|[0.9,62.2,60.0,13...|[1.88594492491467...|\n",
      "|  4| 0.42|    Ideal|    F|    VS1| 61.6|         56.0|  4.82|  4.8|  2.96| 1082| 68.48255999999999|[0.42,61.6,56.0,6...|[0.88010763162684...|\n",
      "|  5| 0.31|    Ideal|    F|   VVS1| 60.4|         59.0|  4.35| 4.43|  2.65|  779|51.066824999999994|[0.31,60.4,59.0,5...|[0.64960325191505...|\n",
      "+---+-----+---------+-----+-------+-----+-------------+------+-----+------+-----+------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/27 16:15:10 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , carat, cut, color, clarity, depth, table, x, y, z, price\n",
      " Schema: _c0, carat, cut, color, clarity, depth, table, x, y, z, price\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/exouser/bigdata/cubic_zirconia.csv\n"
     ]
    }
   ],
   "source": [
    "std_data.show(5) #our latest dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bcc72c-ad76-41b8-a5fe-15f6d7acd0cb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Part 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2833178-2e03-4031-9e4f-9ff4a8604376",
   "metadata": {},
   "source": [
    "##### 1.Data Preprocessing for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "86140f71-8ed1-475d-8cdd-1cea26ca1f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting categorical to numerical\n",
    "columns = num_ip+str_ip + ['Price']\n",
    "df = std_data.select(columns)\n",
    "if len(str_ip)!=0:\n",
    "    for col in str_ip:\n",
    "        indexer = StringIndexer(inputCol= col , outputCol=col+\"_num\")\n",
    "        df  = indexer.fit(df).transform(df)\n",
    "        num_ip.append(col+\"_num\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "951989ab-7d5e-4aa3-bd61-8252e4a81302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-------------+------------------+---------+-----+-------+-----+-------+---------+-----------+\n",
      "|Carat|depth|Table_percent|            Volume|      Cut|Color|Clarity|Price|Cut_num|Color_num|Clarity_num|\n",
      "+-----+-----+-------------+------------------+---------+-----+-------+-----+-------+---------+-----------+\n",
      "|  0.3| 62.1|         58.0| 48.72667799999999|    Ideal|    E|    SI1|  499|    0.0|      1.0|        0.0|\n",
      "| 0.33| 60.8|         58.0|53.225640000000006|  Premium|    G|     IF|  984|    1.0|      0.0|        6.0|\n",
      "|  0.9| 62.2|         60.0|        139.726944|Very Good|    E|   VVS2| 6289|    2.0|      1.0|        4.0|\n",
      "| 0.42| 61.6|         56.0| 68.48255999999999|    Ideal|    F|    VS1| 1082|    0.0|      2.0|        3.0|\n",
      "| 0.31| 60.4|         59.0|51.066824999999994|    Ideal|    F|   VVS1|  779|    0.0|      2.0|        5.0|\n",
      "+-----+-----+-------------+------------------+---------+-----+-------+-----+-------+---------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8056787a-b741-44b6-8e57-ac7ebe59afbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Carat',\n",
       " 'depth',\n",
       " 'Table_percent',\n",
       " 'Volume',\n",
       " 'Cut_num',\n",
       " 'Color_num',\n",
       " 'Clarity_num']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = num_ip\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1d78f56d-2707-4ecd-9796-c6550ab2b6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_columns = features + ['Price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d4156024-9eca-47e0-8aa7-de8fec721789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-------------+------------------+-------+---------+-----------+-----+\n",
      "|Carat|depth|Table_percent|            Volume|Cut_num|Color_num|Clarity_num|Price|\n",
      "+-----+-----+-------------+------------------+-------+---------+-----------+-----+\n",
      "|  0.3| 62.1|         58.0| 48.72667799999999|    0.0|      1.0|        0.0|  499|\n",
      "| 0.33| 60.8|         58.0|53.225640000000006|    1.0|      0.0|        6.0|  984|\n",
      "|  0.9| 62.2|         60.0|        139.726944|    2.0|      1.0|        4.0| 6289|\n",
      "| 0.42| 61.6|         56.0| 68.48255999999999|    0.0|      2.0|        3.0| 1082|\n",
      "| 0.31| 60.4|         59.0|51.066824999999994|    0.0|      2.0|        5.0|  779|\n",
      "| 1.02| 61.5|         56.0|167.28234600000002|    0.0|      4.0|        1.0| 9502|\n",
      "| 1.01| 63.7|         60.0|         161.22015|    3.0|      3.0|        0.0| 4836|\n",
      "|  0.5| 61.5|         62.0|         80.356848|    1.0|      1.0|        0.0| 1415|\n",
      "| 1.21| 63.8|         64.0|189.79833599999998|    3.0|      3.0|        0.0| 5407|\n",
      "| 0.35| 60.5|         57.0| 57.38591999999999|    0.0|      2.0|        1.0|  706|\n",
      "| 0.32| 61.6|         56.0|53.018240000000006|    0.0|      1.0|        1.0|  637|\n",
      "|  1.1| 60.7|         55.0|        184.519632|    1.0|      4.0|        0.0| 6468|\n",
      "|  0.5| 61.1|         58.2| 80.88985600000001|    3.0|      1.0|        3.0| 1932|\n",
      "| 0.71| 61.6|         55.0|117.04089599999999|    0.0|      4.0|        2.0| 2767|\n",
      "|  1.5| 66.2|         53.0|236.92512000000002|    4.0|      0.0|        1.0|10644|\n",
      "| 0.31| 61.6|         55.0|51.797610000000006|    0.0|      0.0|        1.0|  544|\n",
      "| 0.34| 61.2|         57.0|57.425903999999996|    0.0|      0.0|        0.0|  650|\n",
      "| 1.01| 59.8|         56.0|        164.604572|    0.0|      4.0|        1.0| 7127|\n",
      "|  0.9| 61.9|         64.0|          136.6596|    3.0|      4.0|        0.0| 3567|\n",
      "| 0.54| 60.0|         59.0| 90.25275599999999|    1.0|      0.0|        1.0| 1637|\n",
      "+-----+-----+-------------+------------------+-------+---------+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Lets create a feature vector considering both the categorical(numerical converted data ) and existing numerical data, if required let's scale\n",
    "df.select(final_columns).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "05ff9e05-30fb-4430-bd4d-dfc3a17b8c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-------------+------------------+---------+-----+-------+-----+-------+---------+-----------+--------------------+--------------------+\n",
      "|Carat|depth|Table_percent|            Volume|      Cut|Color|Clarity|Price|Cut_num|Color_num|Clarity_num|            features|  standarad_features|\n",
      "+-----+-----+-------------+------------------+---------+-----+-------+-----+-------+---------+-----------+--------------------+--------------------+\n",
      "|  0.3| 62.1|         58.0| 48.72667799999999|    Ideal|    E|    SI1|  499|    0.0|      1.0|        0.0|[0.3,62.1,58.0,48...|[0.62864830830489...|\n",
      "| 0.33| 60.8|         58.0|53.225640000000006|  Premium|    G|     IF|  984|    1.0|      0.0|        6.0|[0.33,60.8,58.0,5...|[0.69151313913538...|\n",
      "|  0.9| 62.2|         60.0|        139.726944|Very Good|    E|   VVS2| 6289|    2.0|      1.0|        4.0|[0.9,62.2,60.0,13...|[1.88594492491467...|\n",
      "| 0.42| 61.6|         56.0| 68.48255999999999|    Ideal|    F|    VS1| 1082|    0.0|      2.0|        3.0|[0.42,61.6,56.0,6...|[0.88010763162684...|\n",
      "| 0.31| 60.4|         59.0|51.066824999999994|    Ideal|    F|   VVS1|  779|    0.0|      2.0|        5.0|[0.31,60.4,59.0,5...|[0.64960325191505...|\n",
      "+-----+-----+-------------+------------------+---------+-----+-------+-----+-------+---------+-----------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Feature Vector\n",
    "assembler = VectorAssembler(inputCols=features, outputCol= 'features')\n",
    "df_vect = assembler.transform(df)\n",
    "scaler = StandardScaler(inputCol= 'features' , outputCol = 'standarad_features')\n",
    "standard_model = scaler.fit(df_vect)\n",
    "df_std = standard_model.transform(df_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bff769e-4b08-4795-9f76-7d3dbf17e36b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84d160b-4b78-4732-af37-7d8548250c66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06836ee6-1c02-4e70-953c-40cd21e4e3ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26b6426-3e7d-4cd9-9b88-fb218b294cd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8f4e2625-1693-4176-980d-dcf366535fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|Price|\n",
      "+--------------------+-----+\n",
      "|[0.62864830830489...|  499|\n",
      "|[0.69151313913538...|  984|\n",
      "|[1.88594492491467...| 6289|\n",
      "|[0.88010763162684...| 1082|\n",
      "|[0.64960325191505...|  779|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_final = df_std.select(['standarad_features','Price'])\n",
    "df_final = df_final.withColumnRenamed('standarad_features','features')\n",
    "df_final.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1685dddc-55ca-469c-af37-0f27d4176b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bins  [-inf, 945.0, 2375.0, 5361.0, inf]\n"
     ]
    }
   ],
   "source": [
    "### equal distribution of Price\n",
    "quantiles = df_final.approxQuantile(\"Price\" , [0.25,0.5,0.75],0)\n",
    "bins = [float('-inf')] + quantiles + [float('inf')]\n",
    "print('bins ' , bins)\n",
    "df_stratified = df_final.withColumn(\"Price_bins\", F.when(F.col('Price')<bins[1],0).when((F.col('Price')>bins[1]) & (F.col('Price')<bins[2]),1) \\\n",
    "                            .when((F.col('Price')>bins[2]) & (F.col('Price')<bins[3]),2).otherwise(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5728020a-7862-4737-8591-dd5600872d72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----------+\n",
      "|            features|Price|Price_bins|\n",
      "+--------------------+-----+----------+\n",
      "|[0.62864830830489...|  499|         0|\n",
      "|[0.69151313913538...|  984|         1|\n",
      "|[1.88594492491467...| 6289|         3|\n",
      "|[0.88010763162684...| 1082|         1|\n",
      "|[0.64960325191505...|  779|         0|\n",
      "|[2.13740424823663...| 9502|         3|\n",
      "|[2.11644930462647...| 4836|         2|\n",
      "|[1.04774718050815...| 1415|         1|\n",
      "|[2.53554817682973...| 5407|         3|\n",
      "|[0.73342302635570...|  706|         0|\n",
      "|[0.67055819552521...|  637|         0|\n",
      "|[2.30504379711793...| 6468|         3|\n",
      "|[1.04774718050815...| 1932|         1|\n",
      "|[1.48780099632157...| 2767|         2|\n",
      "|[3.14324154152446...|10644|         3|\n",
      "|[0.64960325191505...|  544|         0|\n",
      "|[0.71246808274554...|  650|         0|\n",
      "|[2.11644930462647...| 7127|         3|\n",
      "|[1.88594492491467...| 3567|         2|\n",
      "|[1.13156695494880...| 1637|         1|\n",
      "+--------------------+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_stratified.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a3d66c46-e9f8-4f30-a051-a69bf010cd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = df_stratified.randomSplit([0.8,0.2],seed= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "427a4b34-48bc-460a-97e4-a5ec0dadbc3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(avg(Price)=3937.0982202341297)]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.agg(F.mean('Price')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d1d91f9d-8177-4950-bc68-6165a7c9cbc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(avg(Price)=3941.474124809741)]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.agg(F.mean('Price')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc90f5c-e40f-4d69-82e8-b6fbdef67f94",
   "metadata": {},
   "source": [
    "both train and test has approximate mean "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b08212d-87e6-4dc8-ade4-a99ce466a6b7",
   "metadata": {},
   "source": [
    "##### 2. Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "ba6ff8ee-a406-48f9-bcb2-46df0428b002",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression, DecisionTreeRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76841279-0fad-4d4e-9973-dca926f22b0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "e24a50e5-e2b9-49da-8b06-4de129bf1bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/27 16:46:28 WARN Instrumentation: [3f33f639] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression(labelCol=\"Price\", featuresCol=\"features\")\n",
    "lr_model = lr.fit(train)\n",
    "lr_predictions = lr_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "76b35844-c591-4feb-8157-e65783fc6b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeRegressor(labelCol=\"Price\", featuresCol=\"features\")\n",
    "dt_model = dt.fit(train)\n",
    "dt_predictions = dt_model.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d49315e-9b93-4f8d-a5db-47bd7a219807",
   "metadata": {},
   "source": [
    "##### 3. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "811845c8-2ae3-4ec1-ae67-972d6b8cf40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression - RMSE: 1519.1445460268512 MAE: 988.3031741653074 R2: 0.8575301806811857\n",
      "Decision Tree - RMSE: 1135.4104504536863 MAE: 620.3638467487106 R2: 0.9204151411476595\n"
     ]
    }
   ],
   "source": [
    "evaluator_rmse = RegressionEvaluator(labelCol=\"Price\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "evaluator_mae = RegressionEvaluator(labelCol=\"Price\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "evaluator_r2 = RegressionEvaluator(labelCol=\"Price\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "\n",
    "# Linear Regression Metrics\n",
    "lr_rmse = evaluator_rmse.evaluate(lr_predictions)\n",
    "lr_mae = evaluator_mae.evaluate(lr_predictions)\n",
    "lr_r2 = evaluator_r2.evaluate(lr_predictions)\n",
    "\n",
    "# Decision Tree Metrics\n",
    "dt_rmse = evaluator_rmse.evaluate(dt_predictions)\n",
    "dt_mae = evaluator_mae.evaluate(dt_predictions)\n",
    "dt_r2 = evaluator_r2.evaluate(dt_predictions)\n",
    "\n",
    "# Display results\n",
    "print(\"Linear Regression - RMSE:\", lr_rmse, \"MAE:\", lr_mae, \"R2:\", lr_r2)\n",
    "print(\"Decision Tree - RMSE:\", dt_rmse, \"MAE:\", dt_mae, \"R2:\", dt_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f229c03-fc74-48e8-975b-9587e6e0b641",
   "metadata": {},
   "source": [
    "Decision Tree Performs better here as we have included both the numerical and numerical converted categorical columns as a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee69617-1480-4df8-9f14-4012f97ba19c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f636868-cc2f-4623-8e08-e7c62d49951d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f8316d4-f813-4807-9c41-f3dc8579dfc1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "####  Bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b9b8ee68-cfb5-4d64-a16b-23059d59161c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/27 16:33:09 WARN CacheManager: Asked to cache already cached data.\n",
      "24/10/27 16:33:09 WARN CacheManager: Asked to cache already cached data.\n",
      "24/10/27 16:33:13 WARN DAGScheduler: Broadcasting large task binary with size 1014.4 KiB\n",
      "24/10/27 16:33:13 WARN DAGScheduler: Broadcasting large task binary with size 1267.7 KiB\n",
      "24/10/27 16:33:14 WARN DAGScheduler: Broadcasting large task binary with size 1531.0 KiB\n",
      "24/10/27 16:33:14 WARN DAGScheduler: Broadcasting large task binary with size 1771.3 KiB\n",
      "24/10/27 16:33:14 WARN DAGScheduler: Broadcasting large task binary with size 1975.3 KiB\n",
      "24/10/27 16:33:14 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/10/27 16:33:14 WARN DAGScheduler: Broadcasting large task binary with size 1347.8 KiB\n",
      "24/10/27 16:33:15 WARN DAGScheduler: Broadcasting large task binary with size 1063.2 KiB\n",
      "24/10/27 16:33:15 WARN DAGScheduler: Broadcasting large task binary with size 1324.4 KiB\n",
      "24/10/27 16:33:15 WARN DAGScheduler: Broadcasting large task binary with size 1597.6 KiB\n",
      "24/10/27 16:33:16 WARN DAGScheduler: Broadcasting large task binary with size 1852.3 KiB\n",
      "24/10/27 16:33:16 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/10/27 16:33:16 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/10/27 16:33:16 WARN DAGScheduler: Broadcasting large task binary with size 1430.1 KiB\n",
      "24/10/27 16:33:17 WARN DAGScheduler: Broadcasting large task binary with size 1120.9 KiB\n",
      "24/10/27 16:33:17 WARN DAGScheduler: Broadcasting large task binary with size 1396.0 KiB\n",
      "24/10/27 16:33:17 WARN DAGScheduler: Broadcasting large task binary with size 1671.5 KiB\n",
      "24/10/27 16:33:17 WARN DAGScheduler: Broadcasting large task binary with size 1918.9 KiB\n",
      "24/10/27 16:33:18 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/10/27 16:33:18 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/10/27 16:33:18 WARN DAGScheduler: Broadcasting large task binary with size 1470.0 KiB\n",
      "24/10/27 16:33:19 WARN DAGScheduler: Broadcasting large task binary with size 1014.4 KiB\n",
      "24/10/27 16:33:19 WARN DAGScheduler: Broadcasting large task binary with size 1267.7 KiB\n",
      "24/10/27 16:33:19 WARN DAGScheduler: Broadcasting large task binary with size 1531.0 KiB\n",
      "24/10/27 16:33:19 WARN DAGScheduler: Broadcasting large task binary with size 1771.3 KiB\n",
      "24/10/27 16:33:19 WARN DAGScheduler: Broadcasting large task binary with size 1975.3 KiB\n",
      "24/10/27 16:33:19 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/10/27 16:33:20 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/10/27 16:33:20 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "24/10/27 16:33:20 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "24/10/27 16:33:20 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "24/10/27 16:33:20 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "24/10/27 16:33:20 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "24/10/27 16:33:20 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "24/10/27 16:33:20 WARN DAGScheduler: Broadcasting large task binary with size 1433.7 KiB\n",
      "24/10/27 16:33:21 WARN DAGScheduler: Broadcasting large task binary with size 1063.2 KiB\n",
      "24/10/27 16:33:21 WARN DAGScheduler: Broadcasting large task binary with size 1324.3 KiB\n",
      "24/10/27 16:33:22 WARN DAGScheduler: Broadcasting large task binary with size 1597.6 KiB\n",
      "24/10/27 16:33:22 WARN DAGScheduler: Broadcasting large task binary with size 1852.3 KiB\n",
      "24/10/27 16:33:22 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/10/27 16:33:22 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/10/27 16:33:22 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "24/10/27 16:33:22 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "24/10/27 16:33:22 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "24/10/27 16:33:22 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "24/10/27 16:33:23 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "24/10/27 16:33:23 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "24/10/27 16:33:23 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "24/10/27 16:33:23 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "24/10/27 16:33:23 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "24/10/27 16:33:23 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "24/10/27 16:33:23 WARN DAGScheduler: Broadcasting large task binary with size 1553.2 KiB\n",
      "24/10/27 16:33:24 WARN DAGScheduler: Broadcasting large task binary with size 1120.9 KiB\n",
      "24/10/27 16:33:24 WARN DAGScheduler: Broadcasting large task binary with size 1396.0 KiB\n",
      "24/10/27 16:33:25 WARN DAGScheduler: Broadcasting large task binary with size 1671.5 KiB\n",
      "24/10/27 16:33:25 WARN DAGScheduler: Broadcasting large task binary with size 1918.9 KiB\n",
      "24/10/27 16:33:25 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/10/27 16:33:25 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/10/27 16:33:25 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "24/10/27 16:33:25 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "24/10/27 16:33:25 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "24/10/27 16:33:26 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "24/10/27 16:33:26 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "24/10/27 16:33:26 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "24/10/27 16:33:26 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "24/10/27 16:33:26 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "24/10/27 16:33:26 WARN DAGScheduler: Broadcasting large task binary with size 1579.3 KiB\n",
      "24/10/27 16:33:32 WARN DAGScheduler: Broadcasting large task binary with size 1222.9 KiB\n",
      "24/10/27 16:33:33 WARN DAGScheduler: Broadcasting large task binary with size 1484.2 KiB\n",
      "24/10/27 16:33:33 WARN DAGScheduler: Broadcasting large task binary with size 1730.7 KiB\n",
      "24/10/27 16:33:33 WARN DAGScheduler: Broadcasting large task binary with size 1929.8 KiB\n",
      "24/10/27 16:33:33 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/10/27 16:33:33 WARN DAGScheduler: Broadcasting large task binary with size 1321.9 KiB\n",
      "24/10/27 16:33:34 WARN DAGScheduler: Broadcasting large task binary with size 1045.7 KiB\n",
      "24/10/27 16:33:34 WARN DAGScheduler: Broadcasting large task binary with size 1308.8 KiB\n",
      "24/10/27 16:33:34 WARN DAGScheduler: Broadcasting large task binary with size 1575.7 KiB\n",
      "24/10/27 16:33:34 WARN DAGScheduler: Broadcasting large task binary with size 1822.4 KiB\n",
      "24/10/27 16:33:34 WARN DAGScheduler: Broadcasting large task binary with size 2028.2 KiB\n",
      "24/10/27 16:33:35 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/10/27 16:33:35 WARN DAGScheduler: Broadcasting large task binary with size 1409.2 KiB\n",
      "24/10/27 16:33:36 WARN DAGScheduler: Broadcasting large task binary with size 1086.7 KiB\n",
      "24/10/27 16:33:36 WARN DAGScheduler: Broadcasting large task binary with size 1347.5 KiB\n",
      "24/10/27 16:33:36 WARN DAGScheduler: Broadcasting large task binary with size 1615.3 KiB\n",
      "24/10/27 16:33:36 WARN DAGScheduler: Broadcasting large task binary with size 1859.3 KiB\n",
      "24/10/27 16:33:36 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/10/27 16:33:37 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/10/27 16:33:37 WARN DAGScheduler: Broadcasting large task binary with size 1425.7 KiB\n",
      "24/10/27 16:33:38 WARN DAGScheduler: Broadcasting large task binary with size 1222.9 KiB\n",
      "24/10/27 16:33:38 WARN DAGScheduler: Broadcasting large task binary with size 1484.2 KiB\n",
      "24/10/27 16:33:38 WARN DAGScheduler: Broadcasting large task binary with size 1730.7 KiB\n",
      "24/10/27 16:33:38 WARN DAGScheduler: Broadcasting large task binary with size 1929.8 KiB\n",
      "24/10/27 16:33:38 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/10/27 16:33:38 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/10/27 16:33:38 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/10/27 16:33:38 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "24/10/27 16:33:39 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "24/10/27 16:33:39 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "24/10/27 16:33:39 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "24/10/27 16:33:39 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "24/10/27 16:33:39 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "24/10/27 16:33:39 WARN DAGScheduler: Broadcasting large task binary with size 1407.6 KiB\n",
      "24/10/27 16:33:40 WARN DAGScheduler: Broadcasting large task binary with size 1045.7 KiB\n",
      "24/10/27 16:33:40 WARN DAGScheduler: Broadcasting large task binary with size 1308.8 KiB\n",
      "24/10/27 16:33:40 WARN DAGScheduler: Broadcasting large task binary with size 1575.7 KiB\n",
      "24/10/27 16:33:40 WARN DAGScheduler: Broadcasting large task binary with size 1822.4 KiB\n",
      "24/10/27 16:33:41 WARN DAGScheduler: Broadcasting large task binary with size 2028.2 KiB\n",
      "24/10/27 16:33:41 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/10/27 16:33:41 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "24/10/27 16:33:41 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "24/10/27 16:33:41 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "24/10/27 16:33:41 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "24/10/27 16:33:41 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "24/10/27 16:33:41 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "24/10/27 16:33:42 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "24/10/27 16:33:42 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "24/10/27 16:33:42 WARN DAGScheduler: Broadcasting large task binary with size 1522.7 KiB\n",
      "24/10/27 16:33:43 WARN DAGScheduler: Broadcasting large task binary with size 1086.7 KiB\n",
      "24/10/27 16:33:43 WARN DAGScheduler: Broadcasting large task binary with size 1347.5 KiB\n",
      "24/10/27 16:33:43 WARN DAGScheduler: Broadcasting large task binary with size 1615.3 KiB\n",
      "24/10/27 16:33:43 WARN DAGScheduler: Broadcasting large task binary with size 1859.3 KiB\n",
      "24/10/27 16:33:43 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "24/10/27 16:33:44 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/10/27 16:33:44 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "24/10/27 16:33:44 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "24/10/27 16:33:44 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "24/10/27 16:33:44 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "24/10/27 16:33:44 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "24/10/27 16:33:44 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "24/10/27 16:33:44 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "24/10/27 16:33:45 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "24/10/27 16:33:45 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "24/10/27 16:33:45 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "24/10/27 16:33:45 WARN DAGScheduler: Broadcasting large task binary with size 1553.6 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\u001b[1m Feature Importances\u001b[0m\n",
      "(Scores add up to 1)\n",
      "Lowest score is the least important\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------------+\n",
      "|feature      |score                |\n",
      "+-------------+---------------------+\n",
      "|Carat        |0.6743371788214425   |\n",
      "|Volume       |0.22642435309812284  |\n",
      "|Clarity_num  |0.0671443951509537   |\n",
      "|Color_num    |0.027835290117624933 |\n",
      "|depth        |0.0015998489023920832|\n",
      "|Cut_num      |0.001411161933810618 |\n",
      "|Table_percent|0.0012477719756532374|\n",
      "+-------------+---------------------+\n",
      "\n",
      "None\n",
      "759.4924163111617\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "regressor = DecisionTreeRegressor(labelCol='Price', predictionCol=\"prediction\")\n",
    "\n",
    "# Build your parameter grid\n",
    "paramGrid = (ParamGridBuilder() \\\n",
    "                         .addGrid(regressor.maxDepth, [2, 5, 10, 20, 30]) \\\n",
    "                         .addGrid(regressor.maxBins, [10, 20, 40]) \\\n",
    "                         .build())\n",
    "\n",
    "#Cross Validator requires all of the following parameters:\n",
    "crossval = CrossValidator(estimator=regressor,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=RegressionEvaluator(labelCol='Price', predictionCol=\"prediction\",metricName=\"rmse\"),\n",
    "                          numFolds=2)\n",
    "\n",
    "# Fit Model: Run cross-validation, and choose the best set of parameters.\n",
    "fitModel = crossval.fit(train)\n",
    "\n",
    "# Get Best Model\n",
    "DT_BestModel = fitModel.bestModel\n",
    "\n",
    "print(\" \")\n",
    "print('\\033[1m' + \" Feature Importances\"+ '\\033[0m')\n",
    "print(\"(Scores add up to 1)\")\n",
    "print(\"Lowest score is the least important\")\n",
    "print(\" \")\n",
    "DT_featureImportances = DT_BestModel.featureImportances.toArray()\n",
    "# Convert from numpy array to list\n",
    "imp_scores = []\n",
    "for x in DT_featureImportances:\n",
    "    imp_scores.append(float(x))\n",
    "# Then zip with input_columns list and create a df\n",
    "result = spark.createDataFrame(zip(features,imp_scores), schema=['feature','score'])\n",
    "print(result.orderBy(result[\"score\"].desc()).show(truncate=False))\n",
    "\n",
    "# Make predictions.\n",
    "# PySpark will automatically use the best model when you call fitmodel\n",
    "predictions = fitModel.transform(test)\n",
    "\n",
    "\n",
    "eval = RegressionEvaluator(labelCol='Price', predictionCol=\"prediction\",metricName=\"rmse\")\n",
    "# And then apply it your predictions dataframe\n",
    "rmse = eval.evaluate(predictions)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6161d320-4c7b-4131-ad46-0ac69eebc469",
   "metadata": {},
   "source": [
    "From the above it is clear that the Decision Treee regression performs better in predicting the prices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1143a6-7421-4919-baaf-5bfb1059bbda",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Trail \n",
    "Lets remove categorical values (numerical converted) - \n",
    "try the same with removing categorical columns, and check model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "8fa89aca-62de-4b87-a43e-5edf64d5143e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------+-----+\n",
      "|features                                                                     |Price|\n",
      "+-----------------------------------------------------------------------------+-----+\n",
      "|[0.628648308304892,43.953392079062624,25.99991672407327,0.6067842242318457]  |499  |\n",
      "|[0.6915131391353814,43.03327276017725,25.99991672407327,0.6628089580956762]  |984  |\n",
      "|[1.8859449249146765,44.02417048820765,26.89646557662752,1.7399935476686215]  |6289 |\n",
      "|[0.8801076316268489,43.59950003333748,25.10336787151902,0.8528005345041342]  |1082 |\n",
      "|[0.6496032519150552,42.750159123597136,26.448191150350393,0.6359256379351046]|779  |\n",
      "+-----------------------------------------------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#data preprocessing and modelling\n",
    "# features = features[:-3]\n",
    "#Feature Vector\n",
    "assembler = VectorAssembler(inputCols=features, outputCol= 'features')\n",
    "df_vect = assembler.transform(df)\n",
    "#standardisation\n",
    "scaler = StandardScaler(inputCol= 'features' , outputCol = 'standarad_features')\n",
    "standard_model = scaler.fit(df_vect)\n",
    "df_std = standard_model.transform(df_vect)\n",
    "df_final = df_std.select(['standarad_features','Price'])\n",
    "df_final = df_final.withColumnRenamed('standarad_features','features')\n",
    "df_final.show(5,truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "8a6e9667-95ef-4907-96c4-667ae8b2aa88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bins  [-inf, 945.0, 2375.0, 5361.0, inf]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(avg(Price)=3935.6397639668794)]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### equal distribution of Price\n",
    "quantiles = df_final.approxQuantile(\"Price\" , [0.25,0.5,0.75],0)\n",
    "bins = [float('-inf')] + quantiles + [float('inf')]\n",
    "print('bins ' , bins)\n",
    "df_stratified = df_final.withColumn(\"Price_bins\", F.when(F.col('Price')<bins[1],0).when((F.col('Price')>bins[1]) & (F.col('Price')<bins[2]),1) \\\n",
    "                            .when((F.col('Price')>bins[2]) & (F.col('Price')<bins[3]),2).otherwise(3))\n",
    "train,test = df_stratified.randomSplit([0.8,0.2],seed= 1)\n",
    "train.agg(F.mean('Price')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "8caa4f90-caec-471e-80a3-ebc43fa58095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(avg(Price)=3947.305175038052)]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.agg(F.mean('Price')).collect() # both means should be approximately same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "357c63bd-31da-4454-8187-2cfd5a262608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression - RMSE: 1577.4124255136417 MAE: 1027.8491628805155 R2: 0.8466236723058271\n",
      "Decision Tree - RMSE: 1462.2618623929739 MAE: 828.8615641833175 R2: 0.8681991756426296\n"
     ]
    }
   ],
   "source": [
    "#Linear regression and Decision tree comparision\n",
    "lr = LinearRegression(labelCol=\"Price\", featuresCol=\"features\", regParam=0.5)\n",
    "lr_model = lr.fit(train)\n",
    "lr_predictions = lr_model.transform(test)\n",
    "dt = DecisionTreeRegressor(labelCol=\"Price\", featuresCol=\"features\")\n",
    "dt_model = dt.fit(train)\n",
    "dt_predictions = dt_model.transform(test)\n",
    "evaluator_rmse = RegressionEvaluator(labelCol=\"Price\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "evaluator_mae = RegressionEvaluator(labelCol=\"Price\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "evaluator_r2 = RegressionEvaluator(labelCol=\"Price\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "\n",
    "# Linear Regression Metrics\n",
    "lr_rmse = evaluator_rmse.evaluate(lr_predictions)\n",
    "lr_mae = evaluator_mae.evaluate(lr_predictions)\n",
    "lr_r2 = evaluator_r2.evaluate(lr_predictions)\n",
    "\n",
    "# Decision Tree Metrics\n",
    "dt_rmse = evaluator_rmse.evaluate(dt_predictions)\n",
    "dt_mae = evaluator_mae.evaluate(dt_predictions)\n",
    "dt_r2 = evaluator_r2.evaluate(dt_predictions)\n",
    "\n",
    "# Display results\n",
    "print(\"Linear Regression - RMSE:\", lr_rmse, \"MAE:\", lr_mae, \"R2:\", lr_r2)\n",
    "print(\"Decision Tree - RMSE:\", dt_rmse, \"MAE:\", dt_mae, \"R2:\", dt_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "24726d2d-4f2c-48a6-bb01-cb5cc781860f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Decision Tree performs well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac997b3-d1f8-48f7-8c73-1b2e3b2f625f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
